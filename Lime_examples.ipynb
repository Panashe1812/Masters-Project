{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Panashe1812/Masters-Project/blob/main/Lime_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGEW42RZTUvV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os, json\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXa-Z7ZVTUvY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkUakiDDTUvZ"
      },
      "outputs": [],
      "source": [
        "img = get_image('./val_/Latino_Hispanic/1024.jpg')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SEJCX3zTUva"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JboSu2YWTUva"
      },
      "outputs": [],
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "#replace the last linear layer from classifier\n",
        "last_layer = nn.Linear(4096,7)\n",
        "model.classifier[6] = last_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUrU8JbfTUvb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0wfGeJ2TUvb"
      },
      "outputs": [],
      "source": [
        "#path2 ='./vgg16_50_epochs.pth'\n",
        "path3 = './vgg16_model_29_epoch.pth'\n",
        "model = (torch.load(path3,map_location=torch.device('cpu')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiqGSzugTUvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLhHz6dzTUvd"
      },
      "outputs": [],
      "source": [
        "def get_image(path):\n",
        "    with open(os.path.abspath(path), 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB') \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynUrIYNTTUve"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_OAze5NTUve"
      },
      "outputs": [],
      "source": [
        "class_map = {0:'Latino_Hispanic',\n",
        "             1:'East Asian',\n",
        "             2:'Indian',\n",
        "             3:'Middle Eastern',\n",
        "             4:'Black',\n",
        "             5:'Southeast Asian',\n",
        "             6:'White'\n",
        "             }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEhmc_CJTUvf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcsfvTSrTUvf"
      },
      "outputs": [],
      "source": [
        "img = get_image('./val_/Latino_Hispanic/1024.jpg')\n",
        "img_t = get_input_tensors(img)\n",
        "logits = model(img_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlgGX6fPTUvg"
      },
      "outputs": [],
      "source": [
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fou7t_noTUvg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0JG9aACTUvh"
      },
      "outputs": [],
      "source": [
        "probs = torch.softmax(logits, dim=1)\n",
        "probs5 = probs.topk(5)\n",
        "tuple((p,c, class_map[c]) for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj5x23olTUvh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCy8El2nTUvh"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPBKPCcDTUvi"
      },
      "outputs": [],
      "source": [
        "def get_pil_transform(): \n",
        "    transf = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224)\n",
        "    ])    \n",
        "\n",
        "    return transf\n",
        "\n",
        "def get_preprocess_transform():\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])     \n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])    \n",
        "\n",
        "    return transf    \n",
        "\n",
        "pill_transf = get_pil_transform()\n",
        "preprocess_transform = get_preprocess_transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKF0cARTTUvj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uclDGhTTUvk"
      },
      "outputs": [],
      "source": [
        "def batch_predict(images):\n",
        "    model.eval()\n",
        "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    batch = batch.to(device)\n",
        "    \n",
        "    logits = model(batch)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return probs.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwmcuNSkTUvl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLTbT8diTUvl"
      },
      "outputs": [],
      "source": [
        "test_image_1241 = batch_predict([pill_transf(img)])\n",
        "class_map[test_pred.squeeze().argmax()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ6mkICCTUvm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzKVNqtbTUvm"
      },
      "outputs": [],
      "source": [
        "def Lime_explain(test_pred):\n",
        "    explainer = lime_image.LimeImageExplainer()\n",
        "    explanation = explainer.explain_instance(np.array(pill_transf(img)), \n",
        "                                             batch_predict, # classification function\n",
        "                                             top_labels=5, \n",
        "                                             hide_color=0, \n",
        "                                             num_samples=3000) # number of images that will be sent to classification function\n",
        "    \n",
        "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
        "    img_boundry1 = mark_boundaries(temp/255.0, mask)\n",
        "    plt.imshow(img_boundry1)\n",
        "    plt.title('Hispanic Class, 1000 samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4_PE14mTUvn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QUH5eJoTUvn"
      },
      "outputs": [],
      "source": [
        "def apply_mask(explanation)\n",
        "\n",
        "\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
        "img_boundry1 = mark_boundaries(temp/255.0, mask)\n",
        "plt.imshow(img_boundry1)\n",
        "plt.title('Hispanic Class, 1000 samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0yBs8nqTUvo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOsPG_xMTUvo"
      },
      "outputs": [],
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
        "img_boundry2 = mark_boundaries(temp/255.0, mask)\n",
        "plt.imshow(img_boundry2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnZGlL1rTUvp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si64lPKeTUvp"
      },
      "outputs": [],
      "source": [
        "class_light_skin =[ 3850, 4631, 7061, 5477, 4399, 6435, 7038 ,7276, 8883, 9438, 2402,3691, 3752 ,5911 ,5915 ,3935 ,3962 ,5291 ,5480, 6297, 6483, 6435, 7038 ,7197 ,7250, 7415,8291]\n",
        "\n",
        "class_dark_skin = [9112, 9487, 9488, 9306,9306, 9765, 10630, 11299, 11948, 2634, 3119, 2634, 3695, 7607, 5171, 7327, 9988, 2658, 11727,10001]\n",
        "hisp_misclas = [class_light_skin,class_dark_skin]\n",
        "path =\"\"\n",
        "\n",
        "for typ in hisp_misclas:\n",
        "    for idx in typ:\n",
        "        path = \"./train_/Latino_Hispanic/\"+ str(idx)+\".jpg\"\n",
        "        img =get_image(path)\n",
        "        saliency(img, model)\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjyZsmyeTUvp"
      },
      "outputs": [],
      "source": [
        "img1 = get_image('./val_/Latino_Hispanic/1024.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm87JbKrTUvq"
      },
      "outputs": [],
      "source": [
        "test_pred = batch_predict([pill_transf(img)])\n",
        "class_map[test_pred.squeeze().argmax()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufQMgb9ATUvr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQUyvT9FTUvs"
      },
      "outputs": [],
      "source": [
        "# image paths\n",
        "path = \"./Gradcam vis/test images/\"\n",
        "img_1241 = \"1241.jpg\"\n",
        "path_1241= path + img_1241"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBRn5OC_TUvt"
      },
      "outputs": [],
      "source": [
        "img_1241= get_image(path_1241)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY8bnvt-TUvu"
      },
      "outputs": [],
      "source": [
        "test_img_pred = batch_predict([pill_transf(img_1241)])\n",
        "class_map[test_img_pred.squeeze().argmax()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sBgRX5xTUvu"
      },
      "outputs": [],
      "source": [
        "Lime_explain(test_img_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXnardIsTUvu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv_v4yIiTUvv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oczrVRBeTUvv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}